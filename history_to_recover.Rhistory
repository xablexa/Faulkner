# Add the OA codes back onto the data frame as row names
rownames(OAC_Input_PCT_RATIO_IHS_01) <- OAC_Input_PCT_RATIO$OA
library(ggplot2)
# Create a new empty numeric object to store the wss results
wss <- numeric()
# Run k means for 2-12 clusters and store the wss results
for (i in 2:12) wss[i] <- sum(kmeans(OAC_Input_PCT_RATIO_IHS_01, centers=i,nstart=20)$withinss)
# Create a data frame with the results, adding a further column for the cluster number
wss <- data.frame(2:12,wss[-1])
# Plot the results
names(wss) <- c("k","Twss")
ggplot(data=wss, aes(x= k, y=Twss)) + geom_path() + geom_point() + scale_x_continuous(breaks=2:12) + labs(y = "Total within sum of squares")
cluster_7 <- kmeans(x=OAC_Input_PCT_RATIO_IHS_01, centers=7, iter.max=1000000, nstart=10000)
# Load cluster object
load("C:/Users/Axel/Downloads/UrbanAnalyticsCourse-main/UrbanAnalyticsCourse-main/notebooks/data/cluster_7.Rdata")
# Show object content
str(cluster_7)
# Lookup Table
lookup <- data.frame(cluster_7$cluster)
# Add OA codes
lookup$OA <- rownames(lookup)
colnames(lookup) <- c("K_7","OA")
# Recode clusters as letter
lookup$SUPER <- LETTERS[lookup$K_7]
table(lookup$K_7)
# Load packages
library(rgdal)
library(tmap)
# Import OA boundaries
liverpool_SP <- readOGR("C:/Users/Axel/Downloads/UrbanAnalyticsCourse-main/UrbanAnalyticsCourse-main/notebooks/data/Liverpool_OA_2011.geojson")
# Merge lookup
liverpool_SP <- merge(liverpool_SP, lookup, by.x="oa_code",by.y="OA")
m <- tm_shape(liverpool_SP, projection=27700) +
tm_polygons(col="SUPER", border.col = "grey50",   palette="Set1",border.alpha = .3, title="Cluster", showNA=FALSE) +
tm_layout(legend.position = c("left", "bottom"), frame = FALSE)
#Create leaflet plot
tmap_options(check.and.fix = TRUE)
tmap_leaflet(m)
# Merge Original Data (inc. denominators)
LiVOAC_Lookup_Input <- merge(lookup,OAC_Input,by="OA",all.x=TRUE)
# Remove Ratio Variables
LiVOAC_Lookup_Input$k007 <- NULL
LiVOAC_Lookup_Input$k035 <- NULL
# Create Aggregations by SuperGroup
SuperGroup <-aggregate(LiVOAC_Lookup_Input[,4:61], by=list(LiVOAC_Lookup_Input$SUPER),  FUN=sum)
# Create a data frame that will be used to append the index scores
G_Index <- data.frame(SUPER=LETTERS[1:7])
# Loop
for (n in 1:nrow(K_Var)){
num <- paste(K_Var[n,"VariableCode"]) # Get numerator name
den <- paste(K_Var[n,"Denominator"]) # Get denominator name
tmp <- data.frame(round((SuperGroup[,num] / SuperGroup[,den]) / (sum(SuperGroup[,num])/sum(SuperGroup[,den]))*100)) # Calculate index score - these are also rounded
colnames(tmp) <- num
G_Index <- cbind(G_Index,tmp) # Append the index calculations
# Remove temporary objects
remove(list = c("tmp","num","den"))
}
# Merge Original Data (inc. denominators)
LiVOAC_Lookup_Input <- merge(lookup,OAC_Input,by="OA",all.x=TRUE)
# Remove Ratio Variables
LiVOAC_Lookup_Input$k007 <- NULL
LiVOAC_Lookup_Input$k035 <- NULL
# Create Aggregations by SuperGroup
SuperGroup <-aggregate(LiVOAC_Lookup_Input[,4:78], by=list(LiVOAC_Lookup_Input$SUPER),  FUN=sum)
# Create a data frame that will be used to append the index scores
G_Index <- data.frame(SUPER=LETTERS[1:7])
# Loop
for (n in 1:nrow(K_Var)){
num <- paste(K_Var[n,"VariableCode"]) # Get numerator name
den <- paste(K_Var[n,"Denominator"]) # Get denominator name
tmp <- data.frame(round((SuperGroup[,num] / SuperGroup[,den]) / (sum(SuperGroup[,num])/sum(SuperGroup[,den]))*100)) # Calculate index score - these are also rounded
colnames(tmp) <- num
G_Index <- cbind(G_Index,tmp) # Append the index calculations
# Remove temporary objects
remove(list = c("tmp","num","den"))
}
# View the index scores
G_Index
# Convert from wide to narrow format
G_Index_Melt <- melt(G_Index, id.vars="SUPER")
# View the top of the new narrow formatted data frame
head(G_Index_Melt)
# Recode the index scores into aggregate groupings
G_Index_Melt$band <- ifelse(G_Index_Melt$value <= 80,"< 80",ifelse(G_Index_Melt$value > 80 & G_Index_Melt$value <= 120,"80-120",">120"))
# Add a column with short descriptions of the variables
short <- read.csv("C:/Users/Axel/Downloads/UrbanAnalyticsCourse-main/UrbanAnalyticsCourse-main/notebooks/data/OAC_Input_Lookup_short_labels.csv")
G_Index_Melt <- merge(G_Index_Melt,short,by.x="variable",by.y="VariableCode",all.x=TRUE)
# Order the created factors appropriately - needed to ensure the legend and axis make sense in ggolot2
G_Index_Melt$band <- factor(G_Index_Melt$band, levels = c("< 80","80-120",">120"))
G_Index_Melt$VariableDescription <- factor(G_Index_Melt$VariableDescription, levels = short$VariableDescription)
library(ggplot2)
p <- ggplot(G_Index_Melt, aes(x=SUPER, y=VariableDescription, label=value, fill=band)) +
scale_fill_manual(name = "Band",values = c("#EB753B","#F7D865","#B3D09F")) +
scale_x_discrete(position = "top") +
geom_tile(alpha=0.8) +
geom_text(colour="black")
p
knitr::opts_chunk$set(echo = TRUE)
# Load input data
data <- read.csv("data.csv")
# Load input data
data <- read.csv("BPS2_Geog.csv")
head(data)
# Load input data
data <- read.csv("ahahv2domainsindex.csv")
head(data)
# Load input data
data <- read.csv("allvariableslsoawdeciles.csv")
head(data)
# Load input data
data <- read.csv("ahahv2domainsindex.csv")
head(data)
View(data)
# Load input data
data <- read.csv("allvariableslsoawdeciles.csv")
head(data)
View(data)
# Ranking
data$gpp_rank <- rank(data$gpp_dist,ties.method= "min")
data$ed_rank <- rank(data$ed_dist,ties.method= "min")
data$dent_rank <- rank(data$dent_dist,ties.method= "min")
data$pharm_rank <- rank(data$pharm_dist,ties.method= "min")
data$leis_rank <- rank(data$leis_dist,ties.method= "min")
data$gamb_rank <- rank(data$gamb_dist,ties.method= "min")
data$ffood_rank <- rank(data$ffood_dist,ties.method= "min")
data$pubs2_rank <- rank(data$pubs2_dist,ties.method= "min")
# Ranking
data$gpp_rank <- rank(data$gpp_dist,ties.method= "min")
data$ed_rank <- rank(data$ed_dist,ties.method= "min")
data$dent_rank <- rank(data$dent_dist,ties.method= "min")
data$pharm_rank <- rank(data$pharm_dist,ties.method= "min")
data$leis_rank <- rank(data$leis_dist,ties.method= "min")
data$gamb_rank <- rank(data$gamb_dist,ties.method= "min")
data$ffood_rank <- rank(data$ffood_dist,ties.method= "min")
data$pubs2_rank <- rank(data$pubs_dist,ties.method= "min")
data$off2_rank <- rank(data$off2_dist,ties.method= "min")
# Ranking
data$gpp_rank <- rank(data$gpp_dist,ties.method= "min")
data$ed_rank <- rank(data$ed_dist,ties.method= "min")
data$dent_rank <- rank(data$dent_dist,ties.method= "min")
data$pharm_rank <- rank(data$pharm_dist,ties.method= "min")
data$leis_rank <- rank(data$leis_dist,ties.method= "min")
data$gamb_rank <- rank(data$gamb_dist,ties.method= "min")
data$ffood_rank <- rank(data$ffood_dist,ties.method= "min")
data$pubs2_rank <- rank(data$pubs_dist,ties.method= "min")
data$off2_rank <- rank(data$off_dist,ties.method= "min")
data$tobac_rank<- rank(data$tobac_dist,ties.method= "min")
data$g900_rank <- rank(data$green_pas,ties.method= "min")
data$no2_rank <- rank(data$no2_mean,ties.method= "min")
data$pm10_rank <- rank(data$pm10_mean,ties.method= "min")
data$so2_rank <- rank(data$so2_mean,ties.method= "min")
# Invert ranking
data$gamb_rank <- rank(-data$gamb_rank)
data$ffood_rank <- rank(-data$ffood_rank)
data$pubs2_rank <- rank(-data$pubs2_rank)
data$off2_rank <- rank(-data$off2_rank)
data$tobac_rank <- rank(-data$tobac_rank)
data$g900_rank <- rank(-data$g900_rank)
# FUNC: Rankit rank-based normalisation
norm_default <- function(x,y){(x-0.5)/nrow(y)}
# Rankit rank-based normalisation
data$gpp_norm <- norm_default(data$gpp_rank, data)
data$gpp_norm <- qnorm(data$gpp_norm, mean = 0, sd = 1)
data$ed_norm <- norm_default(data$ed_rank, data)
data$ed_norm <- qnorm(data$ed_norm, mean = 0, sd = 1)
data$dent_norm <- norm_default(data$dent_rank, data)
data$dent_norm <- qnorm(data$dent_norm, mean = 0, sd = 1)
data$pharm_norm <- norm_default(data$pharm_rank, data)
data$pharm_norm <- qnorm(data$pharm_norm, mean = 0, sd = 1)
data$gamb_norm <- norm_default(data$gamb_rank, data)
data$gamb_norm <- qnorm(data$gamb_norm, mean = 0, sd = 1)
data$ffood_norm <- norm_default(data$ffood_rank, data)
data$ffood_norm <- qnorm(data$ffood_norm, mean = 0, sd = 1)
data$pubs2_norm <- norm_default(data$pubs2_rank, data)
data$pubs2_norm <- qnorm(data$pubs2_norm, mean = 0, sd = 1)
data$leis_norm <- norm_default(data$leis_rank, data)
data$leis_norm <- qnorm(data$leis_norm, mean = 0, sd = 1)
data$g900_norm <- norm_default(data$g900_rank, data)
data$g900_norm <- qnorm(data$g900_norm, mean = 0, sd = 1)
data$off2_norm <- norm_default(data$off2_rank, data)
data$off2_norm <- qnorm(data$off2_norm, mean = 0, sd = 1)
data$tobac_norm <- norm_default(data$tobac_rank, data)
data$tobac_norm <- qnorm(data$tobac_norm, mean = 0, sd = 1)
data$no2_norm <- norm_default(data$no2_rank, data)
data$no2_norm <- qnorm(data$no2_norm, mean = 0, sd = 1)
data$pm10_norm <- norm_default(data$pm10_rank, data)
data$pm10_norm <- qnorm(data$pm10_norm, mean = 0, sd = 1)
data$so2_norm <- norm_default(data$so2_rank, data)
data$so2_norm <- qnorm(data$so2_norm, mean = 0, sd = 1)
# Domain scores
data$r_domain <- (0.20 * data$gamb_norm +
0.20 * data$ffood_norm +
0.20 * data$pubs2_norm +
0.20 * data$off2_norm +
0.20 * data$tobac_norm)
data$h_domain <- (0.20 * data$gpp_norm +
0.20 * data$ed_norm +
0.20 * data$dent_norm +
0.20 * data$pharm_norm +
0.20 * data$leis_norm)
data$e_domain <- (0.25 * data$g900_norm +
0.25 * data$no2_norm +
0.25 * data$pm10_norm +
0.25 * data$so2_norm)
# Domain ranks
data$r_rank <- rank(data$r_domain,ties.method= "min")
data$h_rank <- rank(data$h_domain,ties.method= "min")
data$e_rank <- rank(data$e_domain,ties.method= "min")
# FUNC: Exponential transformation
exp_trans <- function(x,y){-23*log(1-(x/nrow(y))*(1-exp(-100/23)), base = exp(1))}
# Exponential transformation
data$r_exp <- exp_trans(data$r_rank,data)
data$h_exp <- exp_trans(data$h_rank,data)
data$e_exp <- exp_trans(data$e_rank,data)
# Composite index scores
data$index_sc <- (0.333 * data$r_exp +
0.334 * data$h_exp +
0.333 * data$e_exp)
# Merge LSOA boundaries to the new index
library(sp)
library(rgdal)
lsoa <- readOGR('.', 'lsoa11')
# Merge LSOA boundaries to the new index
library(sp)
library(rgdal)
lsoa <- readOGR('.', 'LSOA_2011_EW_BFC')
head(lsoa@data)
sdf <- merge(lsoa, data[,c(1,50:53)], 'lsoa11')
# Merge LSOA boundaries to the new index
library(sp)
library(rgdal)
lsoa <- readOGR('.', 'LSOA_2011_EW_BFC')
head(lsoa@data)
sdf <- merge(lsoa, data[,c(1,50:53)], 'LSOA_2011_EW_BFC')
View(lsoa)
# Merge LSOA boundaries to the new index
library(sp)
library(rgdal)
lsoa <- readOGR('.', 'LSOA_2011_EW_BFC')
head(lsoa@data)
sdf <- merge(lsoa, data[,c(1,50:53)], by.x=LSOA11CD, by.y=lsoa11)
# Merge LSOA boundaries to the new index
library(sp)
library(rgdal)
lsoa <- readOGR('.', 'LSOA_2011_EW_BFC')
head(lsoa@data)
sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
class(lsoa@data$LSOA11CD)
class(data$lsoa11)
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa, data[,c(1,50:53)], by.x="LSOA11CD", by.y="lsoa11")
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa, data[,c(1,50:53)], by.x="LSOA11CD", by.y=lsoa11)
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa, data[,c(1,50:53)], by.x="LSOA11CD", by.y=data$lsoa11)
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa, data[,c(1,50:53)], by.x="LSOA11CD", by.y=data$lsoa11)
#df <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x="LSOA11CD", by.y=data$lsoa11)
# Use tMap library for mapping the new index
library(tmap)
library(gridExtra)
sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa, data[,c(1,50:53)], by.x=c("LSOA11CD"), by.y=c("lsoa11"))
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa, data[,c(1,50:53)], by.x=c("LSOA11CD"), by.y=c("data$lsoa11"))
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa, data[,c(1,50:53)], by.x=c("LSOA11CD"), by.y=data$gpp_dist)
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa, data[,c(1,50:53)], by.x=c("LSOA11CD"), by.y=data$lsoa11)
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa, data[,c(1,50:53)], by.x=c("LSOA11CD"), by.y=c("lsoa11"))
class(data$lsoa11)
class(lsoa@data$LSOA11CD)
class(data)
class(lsoa@data)
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa@data, data[,c(1,50:53)], by.x=c("LSOA11CD"), by.y=c("lsoa11"))
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa@data, data[,c(1,50:53)], by.x="LSOA11CD", by.y="lsoa11")
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa@data, data[,c(1,50:53)], by.x="LSOA11CD", by.y="lsoa11")
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa@data, data[,c(1,50:53)], by.x="LSOA11CD", by.y="lsoa11")
data$lsoa11
lsoa@data$LSOA11CD
class(lsoa@data$LSOA11CD)
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
sdf <- merge(lsoa@data, data[,c(1,50:53)], by.x="LSOA11CD", by.y="lsoa11")
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
lsoa@data$LSOA11CD <- as.factor(lsoa@data$LSOA11CD)
data$lsoa11 <- as.factor(data$lsoa11)
sdf <- merge(lsoa@data, data[,c(1,50:53)], by.x="LSOA11CD", by.y="lsoa11")
#sdf <- merge(lsoa, data[,c(1,50:53)], by.x=lsoa@data$LSOA11CD, by.y=lsoa11)
lsoa@data$LSOA11CD <- as.character(lsoa@data$LSOA11CD)
data$lsoa11 <- as.character(data$lsoa11)
sdf <- merge(lsoa@data, data[,c(1,50:53)], by.x="LSOA11CD", by.y="lsoa11")
library(tidyverse)
library(sf)
#install.packages("rnaturalearth")
library(rnaturalearth)
#install.packages("usmap")
library(usmap)
library(stringr)
us <- ne_countries(country="united states of america", type="countries")
us <- st_as_sf(us)
student_pnts <- st_sample(us, 2000)
ggplot() +
geom_sf(data = us) +
geom_sf(data = student_pnts)
us <- st_transform(us, 4326)
student_pnts <- st_transform(student_pnts, 4326)
st_crs(us)
st_crs(student_pnts)
student_pnts2 <- read.csv(file = "C:/Users/Axel/Desktop/GIS Master's Courses/Data Interoperability/term paper sources/student_pnts.csv")
student_pnts2$geometry <- str_sub(student_pnts2$geometry, 1, -2)
student_pnts2$X <- str_sub(student_pnts2$X, 3, -1)
names(student_pnts2)[names(student_pnts2) == "X"] <- "lon"
names(student_pnts2)[names(student_pnts2) == "geometry"] <- "lat"
#student_pnts2$id <- c(1:nrow(student_pnts2))
write.csv(student_pnts2, "C:/Users/Axel/Desktop/GIS Master's Courses/Data Interoperability/term paper sources/student_pnts2.csv")
library(tidyverse)
library(sf)
#install.packages("rnaturalearth")
library(rnaturalearth)
#install.packages("usmap")
library(usmap)
library(stringr)
student_pnts2 <- read.csv(file = "C:/Users/Axel/Desktop/GIS Master's Courses/Data Interoperability/term paper sources/student_pnts.csv")
student_pnts2$geometry <- str_sub(student_pnts2$geometry, 1, -2)
student_pnts2$X <- str_sub(student_pnts2$X, 3, -1)
names(student_pnts2)[names(student_pnts2) == "X"] <- "lon"
names(student_pnts2)[names(student_pnts2) == "geometry"] <- "lat"
#student_pnts2$id <- c(1:nrow(student_pnts2))
write.csv(student_pnts2, "C:/Users/Axel/Desktop/GIS Master's Courses/Data Interoperability/term paper sources/student_pnts2.csv")
# FCC's Census Block Conversions API
# http://www.fcc.gov/developers/census-block-conversions-api
# This product uses the FCC Data API but is not endorsed or certified by the FCC
latlong2fips <- function(latitude, longitude) {
url <- "https://geo.fcc.gov/api/census/block/find?format=json&latitude=%f&longitude=%f"
url <- sprintf(url, latitude, longitude)
json <- RJSONIO::fromJSON(url)
as.character(json$Block['FIPS'])
}
#for loop to iterate through student_pnts2
for (i in 1:nrow(student_pnts2)) {
student_pnts2$Block_ID[i] <- latlong2fips(latitude = student_pnts2$lat[i],
longitude = student_pnts2$lon[i])
}
#let's go ahead and add in the fips code while we're at it!
student_pnts2 <- read.csv(file = "C:/Users/Axel/Desktop/GIS Master's Courses/Data Interoperability/term paper sources/student_pnts2.csv")
# FCC's Census Block Conversions API
# http://www.fcc.gov/developers/census-block-conversions-api
# This product uses the FCC Data API but is not endorsed or certified by the FCC
latlong2fips <- function(latitude, longitude) {
url <- "https://geo.fcc.gov/api/census/block/find?format=json&latitude=%f&longitude=%f"
url <- sprintf(url, latitude, longitude)
json <- RJSONIO::fromJSON(url)
as.character(json$Block['FIPS'])
}
#for loop to iterate through student_pnts2
for (i in 1:nrow(student_pnts2)) {
student_pnts2$Block_ID[i] <- latlong2fips(latitude = student_pnts2$lat[i],
longitude = student_pnts2$lon[i])
}
student_pnts2 <- student_pnts2[nchar(student_pnts2$Block_ID) == 15,] #remove failed values
write.csv(student_pnts2, "C:/Users/Axel/Desktop/GIS Master's Courses/Data Interoperability/term paper sources/student_pnts2.csv")
knitr::opts_chunk$set(echo = TRUE)
library(classInt)
library(RColorBrewer)
library(ggplot2)
library(ggmap)
# Load data
broadband.london <- read.csv("data/bband_london.csv")
# Load data
broadband.london <- read.csv("C:/Users/Axel/Downloads/UrbanAnalyticsCourse-main/UrbanAnalyticsCourse-main/notebooks/data/bband_london.csv")
# Preview
head(broadband.london)
# Get some summary statistics
summary(broadband.london)
# Produce a simple boxplot
boxplot(broadband.london$Median_Downl_Speed,
main = "Median Download Speeds by LSOA, London",
ylab = "Median Download Speed (Mbit/s)")
# Get boxplots statistics
boxplot.stats(broadband.london$Median_Downl_Speed)
# Produce a boxplot with labels
boxplot(broadband.london$Median_Downl_Speed,
main = "Median Download Speeds by LSOA, London",
ylab = "Median Download Speed (Mbit/s)")
text(y = boxplot.stats(broadband.london$Median_Downl_Speed)$stats,
labels = round(boxplot.stats(broadband.london$Median_Downl_Speed)$stats, 1),
x = 1.25) # x controls how far to the right our labels will be plotted
# Boxplots by subregion group
boxplot(Median_Downl_Speed ~ Region, # Formula
data = broadband.london,
main = "Median Download Speeds by LSOA, London Subregions",
xlab = "London Subregion",
ylab = "Median Download Speed (Mbit/s)")
# Boxplots by Borough
par(mar = c(11, 4.1, 4.1, 2.1))
boxplot(formula = Median_Downl_Speed ~ Borough,
data = broadband.london,
main = "Median Download Speeds by LSOA, London Boroughs",
ylab = "Median Download Speed (Mbit/s)",
las = 2) # vertical labels
par(mar = c(5.1, 4.1, 4.1, 2.1))
# Load data
airbnb.dc <- read.csv("data/airbnb_dc.csv")
# Load data
airbnb.dc <- read.csv("C:/Users/Axel/Downloads/UrbanAnalyticsCourse-main/UrbanAnalyticsCourse-main/notebooks/data/airbnb_dc.csv")
# Preview
head(airbnb.dc)
# Data structure
str(airbnb.dc)
# Fix cleaning_fee
airbnb.dc$cleaning_fee <- substring(airbnb.dc$cleaning_fee, 2) # crop the first character ($ sign)
airbnb.dc$cleaning_fee[airbnb.dc$cleaning_fee == ""] <- 0 # assign 0 to empty cells
airbnb.dc$cleaning_fee <- as.numeric(airbnb.dc$cleaning_fee)
# Check the distribution of prices and number of beds
summary(airbnb.dc$price)
summary(airbnb.dc$beds)
# Plot the distribution of price by limiting the x-axis
hist(airbnb.dc$price,
breaks = max(airbnb.dc$price)/10,  # to get a ~ $10 interval
xlim = c(0, 500), # this will limit the x axis values between $0 and $500
main = "Distribution of Airbnb Prices in Washington, DC, 2015", # Graph Title
xlab = "Price ($)", # X-Axis Title
ylab = "Number of Listings") # Y-Axis Title)
# Assign variables for simplicity
x <- airbnb.dc$price
y <- airbnb.dc$beds
# Calculate (Pearson) correlation; note that method = "pearson" is the default, and thus can be omitted
cor(x, y, method = "pearson")
cor(x, y, method = "pearson", use = "complete.obs")
# Assign variables
x <- airbnb.dc$price + airbnb.dc$cleaning_fee # add the two variables
y <- airbnb.dc$beds
cor(x, y, method = "pearson", use = "complete.obs")
# Assign variables
x <- airbnb.dc$price + airbnb.dc$cleaning_fee
y <- airbnb.dc$review_scores_rating
cor(x, y, method = "pearson", use = "complete.obs")
# Assign variables
x <- airbnb.dc$price
y <- airbnb.dc$beds
# Calculate correlation
cor.test(x, y, method = "pearson")
# Make bins of prices
breaks <- classIntervals(airbnb.dc$price, 5, style = "kmeans", dataPrecision = 0)
# Make new variable with categories
airbnb.dc$price_cat <- cut(airbnb.dc$price, breaks = breaks$brks, include.lowest = T)
# Get 5 colors based on the YlGnBu ramp
map_colors <- brewer.pal(5, "YlGnBu")
# Get the Washington, DC general location
dc.location <- c(-77.11, 38.83, -76.93, 38.99)
# Get backdrop map tiles, various sources can be used here such as google, osm, stamen, etc.
dc.map <- get_map(location=dc.location, source="stamen", maptype="toner", zoom=13, crop=FALSE)
# Make a map with the airbnb points
airbnb.dc.map <- ggmap(dc.map) +
geom_point(aes(x=longitude, y=latitude, color=price_cat),
data=airbnb.dc, alpha = 0.9, size = 0.6) +
theme(axis.title=element_blank(), axis.text=element_blank(),
axis.ticks=element_blank(),
panel.border = element_rect(colour="black", fill=NA, size=1.2)) +
scale_color_manual(values = map_colors)
airbnb.dc.map # Plot the map
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(NOAWT=1)
library("RNetLogo")
library("ggplot2")
NLStart("C:/Program Files/NetLogo 6.3.0", gui = FALSE, nl.jarname = "netlogo-6.3.0.jar")
NLStart("C:/Program Files/NetLogo 6.3.0/call", gui = FALSE, nl.jarname = "netlogo-6.3.0.jar")
NLStart("C:/Program Files/NetLogo 6.3.0/call", gui = FALSE, nl.jarname = "netlogo-6.3.0.jar")
NLStart("C:/Program Files/NetLogo 6.3.0", gui = FALSE, nl.jarname = "netlogo-6.3.0.jar")
NLLoadModel("C:/Program Files/NetLogo 6.3.0/models/Sample Models/Social Science/Segregation.nlogo")
NLStart("C:/Program Files/NetLogo 6.3.0", gui = FALSE, nl.jarname = "netlogo-6.3.0.jar")
NLStart("C:/Program Files/NetLogo 6.3.0", gui = FALSE, nl.jarname = "netlogo-6.3.0.jar")
knitr::opts_chunk$set(echo = TRUE)
install.packages("rJava")
install.packages("RNetLogo")
Sys.setenv(NOAWT=1)
library("RNetLogo")
library("ggplot2")
NLStart("C:/Program Files/NetLogo 6.3.0", gui = FALSE, nl.jarname = "netlogo-6.3.0.jar")
knitr::opts_chunk$set(echo = TRUE)
#Sys.setenv(NOAWT=1)
library("RNetLogo")
library("ggplot2")
NLStart("C:/Program Files/NetLogo 6.3.0", gui = FALSE, nl.jarname = "netlogo-6.3.0.jar")
NLStart("C:/Program Files/NetLogo 6.3.0", gui = FALSE, nl.jarname = "netlogo-6.3.0.jar")
NLLoadModel("C:/Program Files/NetLogo 6.3.0/models/Sample Models/Social Science/Segregation.nlogo")
NLStart("C:/Program Files/NetLogo 6.3.0", gui = FALSE, nl.jarname = "netlogo-6.3.0.jar")
knitr::opts_chunk$set(echo = TRUE)
#Sys.setenv(NOAWT=1)
library("RNetLogo")
library("ggplot2")
NLStart("C:/Program Files/NetLogo 5.3.1", gui = FALSE, nl.jarname = "netlogo-5.3.1.jar")
NLStart("C:/Program Files/NetLogo 5.3.1", gui = FALSE, nl.jarname = "netlogo-5.3.1.jar")
NLLoadModel("C:/Program Files/NetLogo 5.3.1/models/Sample Models/Social Science/Segregation.nlogo")
fcc <- read.csv(file = "C:/Users/Axel/Desktop/GIS Master's Courses/Data Interoperability/term paper sources/Fixed_Broadband_Deployment_Data__June_2021_Status_V1.csv")
knitr::opts_chunk$set(echo = TRUE)
#Sys.setenv(NOAWT=1)
library("rJava")
library("RNetLogo")
library("ggplot2")
file.choose()
NLStart("C:\\Program Files\\NetLogo 6.3.0\\app", gui = FALSE, nl.jarname = "netlogo-6.3.0.jar")
getwd()
NLStart("C:\\Program Files\\NetLogo 6.3.0\\app", gui = FALSE, nl.jarname = "netlogo-6.3.0.jar")
savehistory("C:/Users/Axel/Desktop/Psych Stuff/Faulkner/history_to_recover.Rhistory")
